{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Lab-Overview\" data-toc-modified-id=\"Lab-Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Lab Overview</a></div><div class=\"lev2 toc-item\"><a href=\"#Important-things-to-note-about-Python/Jupyter\" data-toc-modified-id=\"Important-things-to-note-about-Python/Jupyter-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Important things to note about Python/Jupyter</a></div><div class=\"lev2 toc-item\"><a href=\"#How-to-load-the-Jupyter-notebook\" data-toc-modified-id=\"How-to-load-the-Jupyter-notebook-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>How to load the Jupyter notebook</a></div><div class=\"lev2 toc-item\"><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Learning Outcomes</a></div><div class=\"lev2 toc-item\"><a href=\"#Set-Up-Libraries\" data-toc-modified-id=\"Set-Up-Libraries-14\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Set Up Libraries</a></div><div class=\"lev1 toc-item\"><a href=\"#Background\" data-toc-modified-id=\"Background-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Background</a></div><div class=\"lev2 toc-item\"><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Introduction</a></div><div class=\"lev1 toc-item\"><a href=\"#Data-Exploration\" data-toc-modified-id=\"Data-Exploration-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data Exploration</a></div><div class=\"lev2 toc-item\"><a href=\"#Displaying-the-data\" data-toc-modified-id=\"Displaying-the-data-31\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Displaying the data</a></div><div class=\"lev2 toc-item\"><a href=\"#Correlation-Matrix\" data-toc-modified-id=\"Correlation-Matrix-32\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Correlation Matrix</a></div><div class=\"lev1 toc-item\"><a href=\"#Building-a-Machine-Learning-Model\" data-toc-modified-id=\"Building-a-Machine-Learning-Model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Building a Machine Learning Model</a></div><div class=\"lev2 toc-item\"><a href=\"#Linear-Regression\" data-toc-modified-id=\"Linear-Regression-41\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Linear Regression</a></div><div class=\"lev2 toc-item\"><a href=\"#Coefficient-of-Determination-(r^2-value)\" data-toc-modified-id=\"Coefficient-of-Determination-(r^2-value)-42\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Coefficient of Determination ($r^2$ value)</a></div><div class=\"lev2 toc-item\"><a href=\"#Over--and-Underfitting\" data-toc-modified-id=\"Over--and-Underfitting-43\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Over- and Underfitting</a></div><div class=\"lev2 toc-item\"><a href=\"#Splitting-Data-into-Training-and-Testing-Sets\" data-toc-modified-id=\"Splitting-Data-into-Training-and-Testing-Sets-44\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Splitting Data into Training and Testing Sets</a></div><div class=\"lev2 toc-item\"><a href=\"#Building-the-Model\" data-toc-modified-id=\"Building-the-Model-45\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Building the Model</a></div><div class=\"lev1 toc-item\"><a href=\"#Feature-Selection-&amp;-Cross-Validation\" data-toc-modified-id=\"Feature-Selection-&amp;-Cross-Validation-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Feature Selection &amp; Cross Validation</a></div><div class=\"lev2 toc-item\"><a href=\"#Polynomial-Regression\" data-toc-modified-id=\"Polynomial-Regression-51\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Polynomial Regression</a></div><div class=\"lev2 toc-item\"><a href=\"#Cross-Validation\" data-toc-modified-id=\"Cross-Validation-52\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Cross Validation</a></div><div class=\"lev2 toc-item\"><a href=\"#Training-and-Testing-Model-on-Best-Feature-Set\" data-toc-modified-id=\"Training-and-Testing-Model-on-Best-Feature-Set-53\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Training and Testing Model on Best Feature Set</a></div><div class=\"lev1 toc-item\"><a href=\"#Making-future-predictions\" data-toc-modified-id=\"Making-future-predictions-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Making future predictions</a></div><div class=\"lev2 toc-item\"><a href=\"#Input-into-existing-model\" data-toc-modified-id=\"Input-into-existing-model-61\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Input into existing model</a></div><div class=\"lev2 toc-item\"><a href=\"#Compare-Predictions-against-Housing-Dataset-1\" data-toc-modified-id=\"Compare-Predictions-against-Housing-Dataset-1-62\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Compare Predictions against Housing Dataset 1</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Overview\n",
    "## Important things to note about Python/Jupyter\n",
    "Welcome to Jupyter. This is a _notebook_ development environment for python. If you've used mathematica before, you'll be familiar with the notebook concept.\n",
    "\n",
    "Each cell (like this one) can either be for _code_ or _markdown_ (a very basic language for formatting text). This cell is a markdown cell.\n",
    "\n",
    "Clicking in the `typing` area of a code cell will let you enter edit mode on that cell. Pressing ctrl+Enter will run the code in that cell. Keyboard shortcuts can be found by clicking on help -> keyboard shortcuts.\n",
    "\n",
    "Please note that any variable created in the code can be viewed by typing `print(variable)`.\n",
    "\n",
    "\n",
    "The point of this exercise is to introduce you to some of the basic concepts of machine learning and data analytics, and apply knowledge of what you've learned in CE3010 to a real-world problem. We use python for this exercise as it is ubiquitous in the world of data science and machine learning, and is good to get some exposure to. However, you will not be graded on your understanding of python or programming in general. You should be able to gain an intuition of what the code is doing by reading through, and all the code is annotated to help you understand what's going on.\n",
    "\n",
    "The lab will be structured as follows:\n",
    "\n",
    "* Lab instructor will go through this jupyter notebook, giving a background to machine learning and taking you through a machine learning problem with the `household_data` dataset\n",
    "* You will then be asked to perform a similar analysis on the `boole_data` dataset, and write up the results.\n",
    "\n",
    "You are allowed to copy and paste or otherwise edit some of the code here to apply it to your own datasets if you are not comfortable writing from scratch. As well as this, 99% of errors or anything else you are having trouble with can be solved by a quick google. In particular, the the [cross-validated](http://stats.stackexchange.com/) and [stack-overflow](http://stackoverflow.com/) stack exchange sites are great resources.\n",
    "\n",
    "## How to load the Jupyter notebook\n",
    "1. Open Chrome and go to https://notebooks.azure.com\n",
    "2. Sign in with your UCC IT account. If you can't remember this, but have a microsoft/hotmail account, you can sign in with this. Otherwise, you'll need to create an account.\n",
    "3. Click on \"Libraries\" in the top left\n",
    "4. Click on \"New Library\", then click on \"From GitHub\"\n",
    "5. The GitHub Repo is https://github.com/lkev/ce3010_lab\n",
    "6. Give it the name \"CE3010 Lab FirstName LastName\" (with your actual name, not FirstName LastName)\n",
    "7. Give it the ID ce3010-firstname-lastname\n",
    "8. Click Import\n",
    "\n",
    "The notebook you'll be using during the class is \"Houseing - Partially Filled\".\n",
    "\n",
    "Notebook for homework is \"Boole - Partially Filled\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Outcomes\n",
    "Data analytics allows implicit, previously unknown, and potentially useful information to be extracted from data. It is an interdisciplinary subfield of computer science that combines artificial intelligence, statistics, machine learning and database research. As the volume of data increases, the proportion of it that people understand decreases significantly. Lying hidden in this data is information that is potentially important but has not yet been discovered or articulated. As a result, data analytics allows the useful information within this data to be successfully accessed and analysed. Data analytics and machine learning have many applications in modern engineering sectors, including:\n",
    "* Building & Energy Analytics\n",
    "* Industrial Manufacturing\n",
    "* Engineering Design\n",
    "* Predictive Maintenance\n",
    "* Fault Detection & Performance Monitoring\n",
    "* Self-driving cars\n",
    "* Banking\n",
    "* Phone typing\n",
    "* Image recognition\n",
    "\n",
    "**The objectives of this assignment include the following:**\n",
    "* To gain an understanding of the concept of data analytics and its application in buildings for energy performance analysis.\n",
    "* Get introduced basic machine learning principles and the importance of having a separate test and training set\n",
    "* Understand the effect of daylight hours, occupancy, heating degree days (HDD) and building opening hours on electrical energy consumption in buildings.\n",
    "* An introduction to data analysis using python, and the sklearn, numpy and pandas libraries\n",
    "* To investigate and analyse the energy performance of a UCC building using both correlation and regression analysis\n",
    "* To predict the future electrical energy performance of the building using the developed regression model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Libraries\n",
    "First, we must import the data and relevant libraries\n",
    "\n",
    "---\n",
    "The following code imports various python packages we need to use, and also imports the Housing data from a .CSV file as the variable `house_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-03T10:12:33.674817Z",
     "start_time": "2017-10-03T10:12:32.830474Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# display plots & graphs in browser:\n",
    "%matplotlib inline\n",
    "\n",
    "# Various libraries that are required\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# set the plot styles\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "\n",
    "# Import house data\n",
    "house_data = pd.read_csv(\"Household Dataset1.csv\")\n",
    "house_data_2 = pd.read_csv(\"Household Dataset2.csv\")\n",
    "\n",
    "def print_full(x):\n",
    "    pd.set_option('display.max_rows', len(x))\n",
    "    pd.set_option('display.max_seq_items', len(x))\n",
    "    display(x)\n",
    "    pd.reset_option('display.max_rows')\n",
    "    pd.reset_option('display.max_seq_items')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Statistics, machine learning and data science can all be thought of as different sides of the same coin. The fields essentially boil down to different applications or uses for statistics, probability and, to some degree, information theory. As well as this, the different fields may have different terms for the same concepts. At the heart of all of these, however, is **data**. The datasets used in this study have been cleaned and prepared for you for easy manipulation.\n",
    "\n",
    "Each column in each dataset represents a number of **features** (also known as **independent variables** or **predictors**), as well as a column representing the **response** (also known as the **output** or **dependent variable**).\n",
    "Each row in the dataset then represents an individual entry, comprising the features and associated response for that sample. A diagram of this can be seen below:\n",
    "\n",
    "<img src=\"data_matrix.png\" alt=\"data matrix\" style=\"width: 450px;\"/>\n",
    "\n",
    "In machine learning, the aim is to build a **model** from existing data i.e., existing features and responses. The model identifies some relationship between the features and the responses, so that with any future data you collect, you can make a good **prediction** (also called **estimate** or **hypothesis**) for what the observed response should be.\n",
    "\n",
    "A good example is house heating: if we have data on a number of houses (the **samples**), we can look at the amount of oil each house used in a year (the **responses**). We then build a model for the relationship between the usage and things such as size, insulation rating, occupancy, etc. (the **features**).\n",
    "\n",
    "Then, when we want to guess what the usage will be for a house with no existing heating oil data, we can **predict** the amount it should use according to our model.\n",
    "\n",
    "There are a large number of different statistical models or algorithms that can be used in machine learning to make predictions. When the prediction must be of a quantitative nature (i.e. where the responses are numeric values), we use a technique known as regression. When the responses are qualitative (i.e. when the responses are certain categories or classes), we use classification. \n",
    "\n",
    "For this assignment, we will be trying to obtain a numeric prediction, so we use regression. Specifically, we will use Ordinary Least Squares Linear Regression, the most basic form of regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "In this exercise we will be trying to predict the heating oil usage of homes on a particular day based on a number of factors, including their insulation rating (1-10, higher is better), the temperature on that day, and the age and size of the home.\n",
    "\n",
    "We have two datasets for houses from two different locations. Dataset 1 includes the heating oil usage for each day, whereas dataset 2 does not.\n",
    "\n",
    "We will build a regression model from dataset 1 by splitting the data into _training_ and _testing_ data. The training data is used to build the model, and the testing data is used to see if it generalizes well. We will also explore a feature extraction method to see if we can improve the performance of our model, and verify this using *K-Fold Cross Validation*.\n",
    "\n",
    "Once we determine the model is performing well, we use it to predict housing heating oil usage for dataset 2. We then compare the two datasets and see why they differ.\n",
    "\n",
    "Before diving straight into the modelling, it is always a good idea to explore the existing data and see how the features relate to one another. This is a useful sanity check for later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying the data\n",
    "Displaying data to get a good idea of what it looks like should always be the first step of any analysis.\n",
    "\n",
    "---\n",
    "\n",
    "The following shows the columns of `house_data`. Note the features, samples and targets.\n",
    "\n",
    "**Note:** The `print_full()` function just prints the data in a better visual format than the standard print function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-03T10:12:37.351383Z",
     "start_time": "2017-10-03T10:12:37.329324Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Total number of samples in data:', )\n",
    "\n",
    "#Display first ten rows of the house_data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, we have 4 features, `insulation`, `temp`, `age` and `home_size`, and one target, `oil_usage`, with a total of 665 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Correlation Matrix\n",
    "Correlation analysis is a very quick but powerful technique that can be used to rapidly see how elements in a dataset interact and correlate with each other. This is particularly useful for the optimisation of building energy data as meaningful correlations between dataset attributes (e.g. HDD, footfall, daylight hours etc.) can be found quickly and effectively. In summary, correlation is a statistical measure of how strong the relationships are between the features and responses in a dataset.\n",
    "\n",
    "Correlation Coefficients between 0 and 1 indicate a **positive correlation**, whereas coefficients between 0 and -1 represent **negative correlation**. A positive correlation coeeficient between two variables means that as one variable rises, the other also rises. A negative correlation means that as one rises, the other falls. The closer a correlation is to 1 or -1, the stronger the correlation, whereas values close to zero indicate little to no statistical relationship between the two variables.\n",
    "\n",
    "<img src=\"correlation_strength.png\" alt=\"correlation strengths\" />\n",
    "\n",
    "---\n",
    "\n",
    "We use the numpy (np) python package to get the correlation coefficients:\n",
    "\n",
    "```python\n",
    "c = np.coeff(X)\n",
    "```\n",
    "\n",
    "This returns a matrix, `c`, of correlation coefficients from an input matrix `X` whose rows are samples and whose columns are features. \n",
    "\n",
    "In the code below, `corr` is an array of correlation values for each column in the `house_data`.\n",
    "\n",
    "**NOTE:** `house_data` is imported with the rows and columns the wrong way around for the `np.corr` function. We need to TRANSPOSE the array by using the `.T` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:37:13.690146Z",
     "start_time": "2017-10-02T20:37:13.319269Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an array of correlation data. Note the .T for transposing\n",
    "corr = np.corrcoef()\n",
    "\n",
    "# Create the axis labels for use in the plot\n",
    "labels = \n",
    "\n",
    "# set the figure size\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot a heatmap to easily visualise the relationships\n",
    "sns.heatmap(corr, xticklabels=labels, yticklabels=labels, annot=True,\n",
    "            cmap='RdBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the darker the colour, the stronger the correlation (red for positive, blue for negative). From this, the following are some of the observations we can make from this graphic:\n",
    "* `insulation` & `oil_usage` are highly negatively correlated (-0.73) – a better insulated house uses proportionately less heating oil, and vice versa\n",
    "* `age` and `oil_usage` are highly positively correlated (0.87) – meaning older houses generally use more heating oil, and newer houses use less heating oil.\n",
    "* `home_size` and `oil_usage` are more weakly correlated correlated - the size of the home has some effect on heating oil usage, but not as much as age, outdoor temperature or insulation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Below, we can see various features plotted against `oil_usage`.\n",
    "\n",
    "As can be seen, the correlation signs and magnitudes match what is seen in the correlation matrix (i.e. strongly/weakly and positively/negatively correlated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:37:22.824961Z",
     "start_time": "2017-10-02T20:37:22.367708Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot age, insulation, home_size against oil_usage\n",
    "sns.pairplot(data=, x_vars=[],\n",
    "             y_vars=[], kind='scatter', size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Machine Learning Model\n",
    "\n",
    "## Linear Regression\n",
    "Linear Regression is a simple but powerful technique that is used for numerical prediction. It is a statistical measure that attempts to identify the strength of correlations between one dependent variable (the response) and a series of other changing instances known as independent variables (the features).\n",
    "\n",
    "Linear regression models are simple models and often provide an adequate and interpretable description of how the inputs affect the outputs. For prediction purposes they can sometimes outperform more elaborate nonlinear models, especially in situations with small numbers of training samples, low signal-to-noise ratio or sparse data.\n",
    "\n",
    "The equation of a linear regression model is simply the sum of the features for a particular sample, except that weights are applied to each feature before summing them together:\n",
    "\n",
    "<center>$\\hat{y} = w_0 + w_1x_1 + w_2x_2 + ... + w_mx_m$</center>\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\hat{y}$ is the prediction/estimate of the linear model for a particular sample (as opposed to $y$, the actual value)\n",
    "* $x_1,x_2, …, x_m$ are the $m$ features of a single sample\n",
    "* $w_0, w_1, …, w_m$ are the $m$ associated weights applied to each feature\n",
    "\n",
    "    The $w_0$ weight is also known as the _intercept_. This is a constant that is added to the equation, and is the value for $\\hat{y}$ if all the features $x$ are equal to zero.\n",
    "\n",
    "Note also that there is no $x_0$ variable. Sometimes, to make mathematical operations easier, $x_0$ is set as 1. This is known as a \"dummy\" variable.\n",
    "\n",
    "In this exercise, we use the popular `sklearn` package in python to perform linear regression.\n",
    "\n",
    "\n",
    "\n",
    "## Coefficient of Determination ($r^2$ value)\n",
    "We evaluate the performance of our model with a particular _scoring metric_, which measures how similar the estimates, $\\hat{y}$, are to the actual responses, $y$. For linear regression, a commonly used metric is the $r^2$ value, which is essentially a similarity score with values between 0 and 1. Typically, \"good\" $r^2$ values are above 0.75.\n",
    "\n",
    "## Over- and Underfitting\n",
    "When training on the data, a low $r^2$ value indicates that the model is not entirely accurate. This is known as **underfitting**. However, a high $r^2$ value does not guarantee that the model will perform well on unseen data as it may **overfit** the data. Examples of these can be seen below:\n",
    "\n",
    "<img src='over_underfitting.png' /img>\n",
    "\n",
    "Each point on these graphs represents a training sample, with a single feature, $X$ (on the x axis), and a target value, $y$, on the y axis. The model itself is shown by the black lines. These are the estimated values of $ŷ$ which will be generated for any unseen values of $X$.\n",
    "\n",
    "In the first example, we see that the model is not capturing the true relationship between the feature and target. This leads to a low $r^2$ score.\n",
    "\n",
    "The third example shows overfitting. In this case, the model would generate a perfect $r^2$ score. However, it is obvious that this is not an accurate reflection of the general relationship between the features and targets.\n",
    "\n",
    "The middle example shows how we would expect to fit the data. This has a lower $r^2$ score on the training data, but is still a better model overall.\n",
    "\n",
    "So, how do we evaluate a model to make sure that it generalises well to unseen data (i.e. the future values we will be predicting)? For this, we split our existing data into a _training_ set and a _testing_ set.\n",
    "\n",
    "## Splitting Data into Training and Testing Sets\n",
    "\n",
    "\n",
    "To build a successful model that generalises well, we need to split up the initial data we use to build and evaluate the model into **training** and **testing** data.\n",
    "\n",
    "The training data is used to build the model. The machine learning algorithm will keep iterating over the samples and making target predictions, and compare these to the real value of the targets. On each iteration it will slightly change various parameters specific to the algorithm in use, and keep iterating until the training predictions it makes closely match the actual training responses.\n",
    "\n",
    "In order to verify that the model performs well on new data that the model has never seen before, we use the testing data to verify. The testing data is fed into the model and a score is given for how similar the test predictions are to the actual test responses.\n",
    "\n",
    "Typically, a ratio of about 80/20 is used for the train/test split.\n",
    "\n",
    "---\n",
    "The code below gets our features, `X`, and the associated targets, `y`.\n",
    "\n",
    "The first 80% of the samples are then used for training, and the rest for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:37:32.054563Z",
     "start_time": "2017-10-02T20:37:32.039518Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a matrix of the features (insulation, temp, age, home_size)\n",
    "X = \n",
    "\n",
    "# Create a vector for the dependent variable/target values (oil_usage)\n",
    "y = \n",
    "\n",
    "# Create an array of indices of the training and testing data. The first ~80%\n",
    "# of the data is used for training, with the rest for testing\n",
    "train_indices = np.arange(0, len(X) * 0.8)\n",
    "test_indices = np.arange(len(X) * 0.8, len(X))\n",
    "\n",
    "# shuffle the data\n",
    "# X, y = shuffle(X, y)\n",
    "\n",
    "# get the training samples\n",
    "X_train = X.iloc[train_indices]\n",
    "y_train = y.iloc[train_indices]\n",
    "\n",
    "# and the test samples\n",
    "X_test = X.iloc[test_indices]\n",
    "y_test = y.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "\n",
    "### Training\n",
    "\n",
    "The linear regression model to predict the heating oil consumption of a dwelling is as follows:\n",
    "\n",
    "<center>$Estimated Heating Oil Use =  w_0+(w_1*InsulationRating)+ (w_2*Temperature)+ (w_3*AgeOfHome)+(w_4*SizeOfHome)$</center>\n",
    "\n",
    "We use statistical packages which use specialised optimisation algorithms to search for and find the best weights. The weights are found by feeding the training data into the model. This training data consists of the measured features, $X_{train}$ and responses, $y_{train}$. The associated weights, $w$, are found by minimising the difference between the estimations, $\\hat{y}_{train}$, and the actual targets, $y_{train}$.\n",
    "\n",
    "---\n",
    "The code below trains a new linear regression model, `oil_use_model`, on the training data, `X_train` and `y_train`. This results in finding the weights, $w$.\n",
    "\n",
    "The intercept, $w_0$, and weights, $w_1, w_2, w_3, w_4$, are printed below. Note that weights are sometimes called _regrsssion coefficients_, which is why they are stored here here in the `.coef_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:37:36.914572Z",
     "start_time": "2017-10-02T20:37:36.905549Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create an instance of the sklearn linear regression model\n",
    "oil_use_model = LinearRegression(fit_intercept=True)\n",
    "\n",
    "# fit the model to the training data\n",
    "oil_use_model.fit()\n",
    "\n",
    "intercept = oil_use_model.intercept_\n",
    "weights = oil_use_model.coef_\n",
    "\n",
    "# the 'format' function here is just to format to 2 decimal places\n",
    "print('intercept (w_0):', intercept)\n",
    "print('weights (w_1, w_2, w_3, w_4):', weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this instance, our linear regression model, `oil_use_model`, consists of the following equation:\n",
    "\n",
    "<center>$Estimated Heating Oil Use =  131.46 - (3.76 * InsulationRating) - (1.19*Temperature) + (2.15*AgeOfHome) + (3.09*SizeOfHome)$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "Once the model is built, we use the weights found previously along with the testing data to verify its real-world performance. As we know, the testing data has a number of samples, again consisting of a number of features, $X_{test}$, and responses, $y_{test}$.\n",
    "\n",
    "We substitute the $X_{test}$ features (home size, insulation, etc.) into our regression model, and multiply by the weights found previously to get an estimated $\\hat{y}_{test}$ for each sample.\n",
    "\n",
    "We can then compare our new $\\hat{y}_{test}$ estimates to our actual measured $y_{test}$ responses to calculate a test set $r^2$ score. This $r^2$ score is the performance we’d expect our model to have with new, unseen, data.\n",
    "\n",
    "---\n",
    "The code below gets predictions, `y_hat`, using the weights stored in our trained model, `oil_use_model`.\n",
    "\n",
    "It then calculates the $r^2$ score using a built in function from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:37:42.295852Z",
     "start_time": "2017-10-02T20:37:42.287858Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find predictions using the test data\n",
    "y_hat = oil_use_model.predict()\n",
    "\n",
    "# get the r2 score\n",
    "\n",
    "score = r2_score()\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the Predictions\n",
    "\n",
    "The resulting score is ~.82, showing that our model fairly accurately models the data.\n",
    "\n",
    "By plotting the predicted $\\hat{y}_{test}$ against the actual $y_{test}$, we can visualise how accurate our model is.\n",
    "\n",
    "---\n",
    "Below we plot `y_hat` against `y_test`. Note that we have re-organised `y_test` here to be in ascending order for easier visualisation. Otherwise the plot looks quite messy (you can play around with this yourself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:37:45.291007Z",
     "start_time": "2017-10-02T20:37:45.037333Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create vector of numbers from 1 to length of y_plot, to serve as the x-axis:\n",
    "nums = np.arange(len(y_hat))\n",
    "\n",
    "# sort the values in ascending order\n",
    "y_test_sorted = np.sort(y_test)\n",
    "y_hat_sorted = y_hat[np.argsort(y_test)]\n",
    "\n",
    "# difference between y_hat and y_test\n",
    "delta = abs(y_hat_sorted - y_test_sorted)\n",
    "\n",
    "# create the plot\n",
    "fig, ax = plt.subplots(figsize = (10,8))\n",
    "\n",
    "# Plot the delta as a line graph\n",
    "ax.plot(nums, delta, label = 'delta (|y_test - y_hat|)')\n",
    "\n",
    "# Plot y_hat\n",
    "ax.scatter(nums, y_hat, label='Predicted/Modelled Values')\n",
    "\n",
    "# plot y_test\n",
    "ax.scatter(nums, y_test, label='Actual Test Set Values')\n",
    "\n",
    "# Set up legend & title\n",
    "fig.suptitle('Plot of y_test vs. y_hat', fontsize='xx-large')\n",
    "ax.legend(fontsize='x-large', frameon=True, shadow=True)\n",
    "ax.set_xlabel('sample no.')\n",
    "ax.set_ylabel('predicted/actual heating oil usage (kWh)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection & Cross Validation\n",
    "A massive part of machine learning is an area called _feature extraction_ and _feature selection_.\n",
    "\n",
    "This involves creating new features from our training data. A simple way to create new features is by introducing **polynomial features** (seen below).\n",
    "\n",
    "## Polynomial Regression\n",
    "By getting the nth degree polynomials of the features for each sample, we can adapt linear regression to model non-linear behaviours.\n",
    "\n",
    "For example, the linear regression model for a sample $X_n$, $X_n = \\{x_1, x_2\\}$ is:\n",
    "\n",
    "$\\hat{y} = w_0 + (w_1*x_1) + (w_2*x_2)$\n",
    "\n",
    "The polynomial features are then:\n",
    "\n",
    "$P^2(X_n) = \\{x_1, x_2, x_1^2, x_1x_2, x_2^2, x_2x_1\\}$\n",
    "\n",
    "This would change our linear regression equation to be:\n",
    "\n",
    "$\\hat{y} = w_0 + (w_1*x_1) + (w_2*x_2) + (w_3*x_1^2) + (w_4*x_2^2) + (w_5*2x_1x_2) $\n",
    "\n",
    "Oftentimes, this can lead to better results.\n",
    "\n",
    "---\n",
    "Below, we get the 2nd degree polynomial features for the first two samples in the training data.\n",
    "\n",
    "This is done with an instance of `PolynomialFeatures` in `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:37:48.520691Z",
     "start_time": "2017-10-02T20:37:48.511666Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_features = X_train[0:2].values\n",
    "print('base features:\\n', base_features)\n",
    "\n",
    "poly_transformer = PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly_features = poly_transformer.fit_transform(base_features)\n",
    "print('\\npoly features:\\n', poly_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "Oftentimes, we want to compare how our model performs on different sets of features. A simple way to do this would be to compare the test set scores of the model trained on each set of features.\n",
    "\n",
    "However, once again this can lead to overfitting, as the best performing set of features may simply just be well fitted to that particular test set; for other samples that set of features may score comparatively worse than the others. In order to avoid this problem, we use **cross validation**.\n",
    "\n",
    "Here, we use a strategy called k-fold cross validation. Essentially, this means that the model is trained k times on k different \"folds\" of the training data, with each fold having a separate training and validation set of its own. This means that for each set of features, k different models are trained, and the average score across these k validation folds are given as the final \"cross validation\" score.\n",
    "\n",
    "<img src='cross_val.png' width='50%'/>\n",
    "\n",
    "The best performing set of features are then used to re-train the model with the full set of data, and tested on the test set.\n",
    "\n",
    "---\n",
    "Below, we perform 5-fold cross validation across polynomial features ranging from 1 (i.e. the base set) to 5.\n",
    "\n",
    "The `KFold().split()` function in `sklearn` allows us to automatically split the data across a number of folds.\n",
    "\n",
    "it is trained on the training portion of each fold, and scored on the validation portion.\n",
    "\n",
    "For each `poly_degree`, we store the average of the score across all folds. The best performing set of features are then scored on the test set to get the final score of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:47:19.691010Z",
     "start_time": "2017-10-02T20:47:19.568685Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly_degrees = []\n",
    "cv_folds = KFold(n_splits=)\n",
    "\n",
    "# set up the scores dictionary\n",
    "scores = {}\n",
    "\n",
    "# loop through each set of features\n",
    "for poly_degree in poly_degrees:\n",
    "    # set up the transformer\n",
    "    poly_transformer = PolynomialFeatures(\n",
    "        degree=poly_degree, include_bias=False)\n",
    "\n",
    "    # generate the new set of features\n",
    "    X_train_poly = poly_transformer.fit_transform()\n",
    "    \n",
    "    # empty scores list\n",
    "    scores[poly_degree] = []\n",
    "    \n",
    "    print('\\nscores for poly_degree={}:'.format(poly_degree))\n",
    "    \n",
    "    for i, (train_indices, validation_indices) in enumerate(\n",
    "            cv_folds.split(X_train, y_train)):\n",
    "        \n",
    "        # get the X and y train and validation set for this fold\n",
    "        X_train_cv = X_train_poly[]\n",
    "        y_train_cv = y_train.iloc[]\n",
    "        \n",
    "        X_valid_cv = X_train_poly[]      \n",
    "        y_valid_cv = y_train.iloc[]\n",
    "        \n",
    "        # build the model and make predictions\n",
    "        oil_use_model = LinearRegression(fit_intercept=True)\n",
    "        oil_use_model.fit(, )\n",
    "        \n",
    "        y_hat = oil_use_model.predict()\n",
    "        \n",
    "        # score it\n",
    "        fold_score = r2_score(, )\n",
    "        \n",
    "        # store the score\n",
    "        scores[poly_degree].append(fold_score)\n",
    "        \n",
    "        print('fold {}: {:.2f}'.format(i, fold_score))\n",
    "    \n",
    "    average_score = np.mean(scores[poly_degree])\n",
    "    print('average across folds: {:.2f}'.format(average_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing Model on Best Feature Set\n",
    "\n",
    "As can be seen, the best set of features was for `poly_degree`=3. In this case, we now train a model on the full set of training data (as opposed to three separate folds), and test on the held-out test set. This should give us a very confident estimate of how the model will perform on new, unseen data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:47:21.760731Z",
     "start_time": "2017-10-02T20:47:21.743688Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly_transformer = PolynomialFeatures(degree=3, include_bias=False)\n",
    "# generate the new set of features\n",
    "X_train_poly = poly_transformer.fit_transform()\n",
    "X_test_poly = poly_transformer.transform()\n",
    "\n",
    "final_oil_use_model = LinearRegression(fit_intercept=True)\n",
    "final_oil_use_model.fit(, )\n",
    "\n",
    "y_hat = final_oil_use_model.predict()\n",
    "\n",
    "score = r2_score(, )\n",
    "\n",
    "print('final score on test set:', score)\n",
    "\n",
    "print('intercept:\\n',  final_oil_use_model.intercept_)\n",
    "print('weights:\\n', final_oil_use_model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an $r^2$ score of ~.94, the model performs very well and seems to be well suited to modelling on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create vector of numbers from 1 to length of y_plot, to serve as the x-axis:\n",
    "nums = np.arange(len(y_hat))\n",
    "\n",
    "# sort the values in ascending order\n",
    "y_test_sorted = np.sort(y_test)\n",
    "y_hat_sorted = y_hat[np.argsort(y_test)]\n",
    "\n",
    "# difference between y_hat and y_test\n",
    "delta = abs(y_hat_sorted - y_test_sorted)\n",
    "\n",
    "# create the plot\n",
    "fig, ax = plt.subplots(figsize = (10,8))\n",
    "\n",
    "# Plot the delta as a line graph\n",
    "ax.plot(nums, delta, label = 'delta (|y_test - y_hat|)')\n",
    "\n",
    "# Plot y_hat\n",
    "ax.scatter(nums, y_hat_sorted, label='Predicted/Modelled Values')\n",
    "\n",
    "# plot y_test\n",
    "ax.scatter(nums, y_test_sorted, label='Actual Test Set Values')\n",
    "\n",
    "# Set up legend & title\n",
    "fig.suptitle('Plot of y_test vs. y_hat', fontsize='xx-large')\n",
    "ax.legend(fontsize='x-large', frameon=True, shadow=True)\n",
    "ax.set_xlabel('sample no.')\n",
    "ax.set_ylabel('predicted/actual heating oil usage (kWh)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making future predictions\n",
    "If the $r^2$ score for the testing data is sufficiently good (~ > 0.75), we can assume the model generalises well to new data.\n",
    "\n",
    "\n",
    "With this in mind, we can feed the model new, unseen features which we do not have associated responses for. We can then use these features to make predictions using our model:\n",
    "<center>$\\hat{y}_{new} = w_0 + w_1x_{1 new} + w_2x_{2 new} + ... + w_mx_{m \\ new}$</center>\n",
    "\n",
    "At this point, since we have a good $r^2$ score on the test set, we can assume that our predictions on a new dataset are accurate. We can then use our predictions to draw conclusions from the data. For example, we can get an average of all $X$ and $y$ from our initial training/testing dataset, and compare to averages of $X_{new}$ and $\\hat{y}_{new}$.\n",
    "If the average of $\\hat{y}_{new}$ is higher than $y$, we can make a reasonable hypothesis of why this is the case by looking at the averages of $X$ and $X_{new}$, and comparing with the correlation matrix found earlier. For example, on the housing dataset, if the average age of houses on the new dataset is higher than on the one used for training and testing, we can infer that the heating oil usage should be higher as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Here we'll import a second housing dataset, with houses that are not included in our previous training data.\n",
    "\n",
    "Note that there is no `oil_usage` column here, so we can make predictions for this based on our previous model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:47:24.249467Z",
     "start_time": "2017-10-02T20:47:24.238463Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "house_data_2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input into existing model\n",
    "\n",
    "Since we now know that the model performs well on unseen data, we can say with some confidence that its predictions will be accurate.\n",
    "\n",
    "---\n",
    "Below, we make predictions from the new data.\n",
    "\n",
    "Note that we must first transform the features with a 3rd-degree polynomial, as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:47:26.499499Z",
     "start_time": "2017-10-02T20:47:26.493479Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new matrix of features - insulation, age, temp, home_size\n",
    "X_new = house_data_2[]\n",
    "\n",
    "X_new_poly = poly_transformer.transform()\n",
    "\n",
    "# Create a vector of actual target values\n",
    "y_pred_new = final_oil_use_model.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Predictions against Housing Dataset 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now got predictions for all the new Data. We can save these predictions as a csv for pasting into excel, or manipulate them from within python.\n",
    "\n",
    "We can sum up and compare the total consumption from each dataset. Why is this higher or lower? In this case, it may be because of increased average temperature, more average occupants per house, or a better insulation rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:47:28.575409Z",
     "start_time": "2017-10-02T20:47:28.551340Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Can export y_pred_new for pasting into the existing Household Dataset1 csv,\n",
    "# for analysis in excel\n",
    "np.savetxt('house_dataset2_predictions.csv', y_pred_new)\n",
    "\n",
    "# Here, we'll perform analysis in python\n",
    "# First, we get the average of all features in dataset1 and _new\n",
    "# The (0) in .mean(0) means go along axis 0, i.e. the columns, instead of axis\n",
    "# 1, i.e. the rows\n",
    "print('means for Dataset1: \\n', house_data.mean(0), '\\n')\n",
    "print('means for Dataset2: \\n', house_data_2.mean(0), '\\n',\n",
    "      'estimated_oil_usage', np.mean(y_pred_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the estimated heating oil usage in dataset 2 is quite low.\n",
    "\n",
    "From the correlation matrix in Section 5 and partial regression plots in 6.3.1, we can see that insulation and average age are strongly positively correlated with Heating Oil usage. These are both lower on average for the houses in dataset2, so we see a lower heating oil usage. Temperature is negatively correlated, so it being higher on average in datset2 will also lower the heating oil usage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "nav_menu": {},
  "notify_time": "10",
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "386px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": "2",
   "toc_cell": true,
   "toc_position": {
    "height": "40px",
    "left": "1560px",
    "right": "20px",
    "top": "133px",
    "width": "239px"
   },
   "toc_section_display": "none",
   "toc_window_display": true,
   "widenNotebook": false
  },
  "toc_position": {
   "height": "1081px",
   "left": "0px",
   "right": "1948px",
   "top": "107px",
   "width": "186px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
